#!/usr/bin/env python3
"""
RAG System Test Suite - Professional Edition
============================================

åŸºäº MIRAGE (ACL 2025) å’Œ RAGBench çš„ä¸“ä¸š RAG è¯„ä¼°æµ‹è¯•å¥—ä»¶

å‚è€ƒæ ‡å‡†:
- MIRAGE: https://arxiv.org/abs/2504.17137
- RAGBench: https://arxiv.org/abs/2407.11005
- mmRAG: https://arxiv.org/abs/2505.11180

åŠŸèƒ½:
- æ£€ç´¢è´¨é‡è¯„ä¼° (Precision, Recall, MRR, NDCG)
- ç”Ÿæˆè´¨é‡è¯„ä¼° (Answer Quality, Context Relevance)
- RAG é€‚åº”æ€§è¯„ä¼° (å™ªå£°å®¹å¿åº¦, ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§)
- ç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•

ä½¿ç”¨æ–¹æ³•:
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    python -m pytest test_rag_system.py -v

    # è¿è¡Œç‰¹å®šæµ‹è¯•
    python -m pytest test_rag_system.py::TestRAGRetrievalMetrics::test_precision_recall -v

    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    python -m pytest test_rag_system.py -v --tb=short --html=report.html
"""

import pytest
import requests
import json
import time
import statistics
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum


# ============================================================
# Test Configuration
# ============================================================

BACKEND_URL = "http://localhost:8000"
MILVUS_URL = "http://localhost:19530"
TIMEOUT = 120  # seconds


# ============================================================
# Industry Standard Benchmarks (MIRAGE, RAGBench)
# ============================================================

# åŸºäº MIRAGE åŸºå‡†çš„è¯„ä¼°æ ‡å‡†
# æ¥æº: https://arxiv.org/abs/2504.17137
MIRAGE_BENCHMARKS = {
    "precision@10": 0.6,       # MIRAGE æ ‡å‡†: 0.6
    "recall@10": 0.5,          # MIRAGE æ ‡å‡†: 0.5
    "mrr": 0.65,              # MIRAGE æ ‡å‡†: 0.65
    "ndcg@10": 0.55,          # MIRAGE æ ‡å‡†: 0.55
    "context_relevance": 0.7, # ä¸Šä¸‹æ–‡ç›¸å…³æ€§æ ‡å‡†
    "answer_quality": 0.6,    # å›ç­”è´¨é‡æ ‡å‡†
    "grounding_score": 0.5,   # äº‹å®ä¾æ®æ ‡å‡†
}

# RAGBench åŸŸç‰¹å®šæµ‹è¯•æŸ¥è¯¢
RAGBENCH_DOMAINS = {
    "finance": [
        "æ–°åŠ å¡å…¬å¸ç¨åŠ¡è¦æ±‚",
        "ODIå¢ƒå¤–æŠ•èµ„å¤‡æ¡ˆæµç¨‹",
        "ACRAå…¬å¸æ³¨å†ŒæŒ‡å—",
    ],
    "legal": [
        "æ–°åŠ å¡EPç­¾è¯è¦æ±‚",
        "PDPAæ•°æ®ä¿æŠ¤åˆè§„",
        "é›‡ä½£æ³•è§„æ³¨æ„äº‹é¡¹",
    ],
    "technology": [
        "åˆ¶é€ ä¸šå‡ºæµ·ä¸œå—äºš",
        "ç§‘æŠ€å…¬å¸å‡ºæµ·ç­–ç•¥",
        "äººå·¥æ™ºèƒ½æŠ•èµ„æœºä¼š",
    ],
}


# ============================================================
# Data Classes
# ============================================================

class TestLevel(Enum):
    """æµ‹è¯•çº§åˆ«"""
    CRITICAL = "critical"      # å…³é”®åŠŸèƒ½æµ‹è¯•
    STANDARD = "standard"    # æ ‡å‡†æ€§èƒ½æµ‹è¯•
    BENCHMARK = "benchmark" # åŸºå‡†å¯¹æ¯”æµ‹è¯•


@dataclass
class RetrievalMetrics:
    """æ£€ç´¢æŒ‡æ ‡"""
    query: str
    total_chunks: int
    unique_sources: int
    scores: List[float]
    relevance_judgments: List[bool]  # Ground truth relevance
    
    # è®¡ç®—æŒ‡æ ‡
    precision: float = 0.0
    recall: float = 0.0
    mrr: float = 0.0
    ndcg: float = 0.0
    f1: float = 0.0
    
    def calculate_metrics(self, k: int = 10):
        """è®¡ç®—æ£€ç´¢æŒ‡æ ‡"""
        if not self.scores:
            return
        
        # æŒ‰åˆ†æ•°æ’åº
        sorted_scores = sorted(self.scores, reverse=True)[:k]
        
        # Precision@K
        if k > 0:
            self.precision = sum(1 for s in sorted_scores if s > 0.5) / k
        
        # Recall@K (å‡è®¾ total relevant = unique_sources)
        if self.unique_sources > 0:
            self.recall = min(1.0, sum(1 for s in sorted_scores if s > 0.5) / self.unique_sources)
        
        # MRR (Mean Reciprocal Rank)
        for i, score in enumerate(sorted_scores, 1):
            if score > 0.5:
                self.mrr = 1.0 / i
                break
        
        # NDCG@K
        dcg = sum((2**int(s > 0.5) - 1) / (i + 1) for i, s in enumerate(sorted_scores))
        idcg = sum(1 / (i + 1) for i in range(min(k, len(sorted_scores))))
        self.ndcg = dcg / idcg if idcg > 0 else 0.0
        
        # F1
        if self.precision + self.recall > 0:
            self.f1 = 2 * self.precision * self.recall / (self.precision + self.recall)


@dataclass
class AnswerQuality:
    """å›ç­”è´¨é‡æŒ‡æ ‡"""
    query: str
    answer: str
    context_chunks: List[str]
    
    # è´¨é‡ç»´åº¦
    length_score: float = 0.0      # é•¿åº¦åˆç†æ€§
    relevance_score: float = 0.0   # ç›¸å…³æ€§
    coherence_score: float = 0.0   # è¿è´¯æ€§
    grounding_score: float = 0.0    # äº‹å®ä¾æ®
    
    def calculate_quality(self):
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        # é•¿åº¦åˆ†æ•° (åˆç†èŒƒå›´: 100-5000 å­—ç¬¦)
        length = len(self.answer)
        if 100 <= length <= 5000:
            self.length_score = 1.0
        elif length < 100:
            self.length_score = length / 100
        else:
            self.length_score = max(0, 1.0 - (length - 5000) / 5000)
        
        # ç›¸å…³æ€§åˆ†æ•° (åŸºäºä¸Šä¸‹æ–‡)
        if self.context_chunks:
            context_text = " ".join(self.context_chunks[:3])
            # ç®€å•ç›¸å…³æ€§: æ£€æŸ¥å›ç­”ä¸­æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡å…³é”®è¯
            common_words = set(self.answer[:200].split()) & set(context_text.split())
            self.relevance_score = min(1.0, len(common_words) / 10)
        
        # è¿è´¯æ€§åˆ†æ•° (åŸºäºå›ç­”é•¿åº¦å’Œç»“æ„)
        sentences = self.answer.count("ã€‚") + self.answer.count(".")
        if sentences > 0:
            self.coherence_score = min(1.0, sentences / 5)
        else:
            self.coherence_score = 0.5
        
        # äº‹å®ä¾æ®åˆ†æ•° (åŸºäºæ˜¯å¦æœ‰ä¸Šä¸‹æ–‡æ”¯æŒ)
        self.grounding_score = 1.0 if self.context_chunks else 0.0


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    name: str
    passed: bool
    level: TestLevel
    duration: float
    message: str = ""
    details: Dict[str, Any] = field(default_factory=dict)
    benchmark_comparison: Dict[str, Any] = field(default_factory=dict)


# ============================================================
# Test Client
# ============================================================

class RAGTestClient:
    """RAG ç³»ç»Ÿæµ‹è¯•å®¢æˆ·ç«¯"""

    def __init__(self, base_url: str = BACKEND_URL):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({"Content-Type": "application/json"})
        self.session.timeout = TIMEOUT

    def get_vector_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡åº“ç»Ÿè®¡"""
        response = self.session.get(f"{self.base_url}/test/vector-stats", timeout=10)
        response.raise_for_status()
        return response.json()

    def get_all_sources(self) -> List[str]:
        """è·å–æ‰€æœ‰æ–‡æ¡£æº (RESTful API v1)"""
        response = self.session.get(f"{self.base_url}/api/v1/sources", timeout=10)
        response.raise_for_status()
        return response.json().get("data", [])

    def get_selected_sources(self) -> List[str]:
        """è·å–å½“å‰é€‰ä¸­çš„æ–‡æ¡£æº (RESTful API v1)"""
        response = self.session.get(f"{self.base_url}/api/v1/selected-sources", timeout=10)
        response.raise_for_status()
        return response.json().get("data", [])

    def set_selected_sources(self, sources: List[str]) -> Dict[str, Any]:
        """è®¾ç½®é€‰ä¸­çš„æ–‡æ¡£æº (RESTful API v1)"""
        response = self.session.post(
            f"{self.base_url}/api/v1/selected-sources",
            json={"sources": sources},
            timeout=10
        )
        response.raise_for_status()
        return response.json()

    def reindex_sources(self, sources: Optional[List[str]] = None) -> Dict[str, Any]:
        """é‡æ–°ç´¢å¼•æ–‡æ¡£æº (RESTful API v1)"""
        payload = {"sources": sources} if sources else {}
        response = self.session.post(
            f"{self.base_url}/api/v1/sources:reindex",
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        return response.json()

    def create_chat(self) -> str:
        """åˆ›å»ºæ–°ä¼šè¯å¹¶è¿”å› chat_id"""
        response = self.session.post(f"{self.base_url}/api/v1/chats", timeout=10)
        response.raise_for_status()
        return response.json().get("data", {}).get("chat_id")

    def get_chat_messages(self, chat_id: str, limit: int = 50) -> List[Dict]:
        """è·å–ä¼šè¯æ¶ˆæ¯"""
        response = self.session.get(
            f"{self.base_url}/api/v1/chats/{chat_id}/messages",
            params={"limit": limit},
            timeout=10
        )
        response.raise_for_status()
        return response.json().get("data", [])

    def get_chat_metadata(self, chat_id: str) -> Dict[str, Any]:
        """è·å–ä¼šè¯å…ƒæ•°æ®"""
        response = self.session.get(f"{self.base_url}/api/v1/chats/{chat_id}/metadata", timeout=10)
        response.raise_for_status()
        return response.json().get("data", {})

    def update_chat_metadata(self, chat_id: str, title: str) -> Dict[str, Any]:
        """æ›´æ–°ä¼šè¯å…ƒæ•°æ®"""
        response = self.session.patch(
            f"{self.base_url}/api/v1/chats/{chat_id}/metadata",
            json={"title": title},
            timeout=10
        )
        response.raise_for_status()
        return response.json().get("data", {})

    def delete_chat(self, chat_id: str) -> Dict[str, Any]:
        """åˆ é™¤ä¼šè¯"""
        response = self.session.delete(f"{self.base_url}/api/v1/chats/{chat_id}", timeout=10)
        response.raise_for_status()
        return response.json().get("data", {})

    def clear_all_chats(self) -> Dict[str, Any]:
        """æ¸…é™¤æ‰€æœ‰ä¼šè¯"""
        response = self.session.delete(f"{self.base_url}/api/v1/chats", timeout=10)
        response.raise_for_status()
        return response.json().get("data", {})

    def test_rag(self, query: str, k: int = 8) -> Dict[str, Any]:
        """æµ‹è¯• RAG æ£€ç´¢"""
        response = self.session.get(
            f"{self.base_url}/test/rag",
            params={"query": query, "k": k},
            timeout=TIMEOUT
        )
        response.raise_for_status()
        return response.json()

    def test_llamaindex_rag(self, query: str, k: int = 10,
                           sources: Optional[List[str]] = None,
                           use_cache: bool = False) -> Dict[str, Any]:
        """æµ‹è¯• LlamaIndex å¢å¼º RAG"""
        payload = {"query": query, "top_k": k, "use_cache": use_cache}
        if sources:
            payload["sources"] = sources

        response = self.session.post(
            f"{self.base_url}/rag/llamaindex/query",
            json=payload,
            timeout=TIMEOUT
        )
        response.raise_for_status()
        return response.json()
    
    def get_llamaindex_stats(self) -> Dict[str, Any]:
        """è·å– LlamaIndex ç»Ÿè®¡"""
        response = self.session.get(f"{self.base_url}/rag/llamaindex/stats", timeout=10)
        response.raise_for_status()
        return response.json()


# ============================================================
# Test Cases
# ============================================================

class TestRAGSystem:
    """RAG ç³»ç»ŸåŸºç¡€æµ‹è¯•"""
    
    @classmethod
    def setup_class(cls):
        cls.client = RAGTestClient()
        cls.results: List[TestResult] = []
    
    def _record_result(self, name: str, passed: bool, level: TestLevel,
                       duration: float, message: str = "", 
                       details: Dict = None, benchmark: Dict = None):
        self.results.append(TestResult(
            name=name,
            passed=passed,
            level=level,
            duration=duration,
            message=message,
            details=details or {},
            benchmark_comparison=benchmark or {}
        ))
    
    def test_vector_stats(self):
        """æµ‹è¯•å‘é‡åº“ç»Ÿè®¡"""
        start = time.time()
        try:
            stats = self.client.get_vector_stats()
            
            assert "collection" in stats
            assert "total_entities" in stats
            assert stats["total_entities"] > 0
            
            duration = time.time() - start
            self._record_result(
                "test_vector_stats", True, TestLevel.CRITICAL, duration,
                f"å‘é‡åº“åŒ…å« {stats['total_entities']} ä¸ªå‘é‡",
                {"entities": stats["total_entities"]}
            )
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_vector_stats", False, TestLevel.CRITICAL, 
                              duration, str(e))
            pytest.fail(f"test_vector_stats failed: {e}")
    
    def test_sources_management(self):
        """æµ‹è¯•æ–‡æ¡£æºç®¡ç†"""
        start = time.time()
        try:
            all_sources = self.client.get_all_sources()
            selected_sources = self.client.get_selected_sources()
            
            assert len(all_sources) > 0
            assert len(selected_sources) > 0
            
            duration = time.time() - start
            self._record_result(
                "test_sources_management", True, TestLevel.CRITICAL, duration,
                f"æ€»æ–‡æ¡£: {len(all_sources)}, å·²é€‰: {len(selected_sources)}",
                {"total": len(all_sources), "selected": len(selected_sources)}
            )
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_sources_management", False, TestLevel.CRITICAL,
                              duration, str(e))
            pytest.fail(f"test_sources_management failed: {e}")


class TestSessionManagement:
    """ä¼šè¯ç®¡ç†æµ‹è¯• (RESTful API v1)"""

    @classmethod
    def setup_class(cls):
        cls.client = RAGTestClient()
        cls.results: List[TestResult] = []
        cls.test_chat_id: Optional[str] = None

    def _record_result(self, name: str, passed: bool, level: TestLevel,
                      duration: float, message: str = "",
                      details: Dict = None, benchmark: Dict = None):
        TestSessionManagement.results.append(TestResult(
            name=name,
            passed=passed,
            level=level,
            duration=duration,
            message=message,
            details=details or {},
            benchmark_comparison=benchmark or {}
        ))

    def test_create_chat(self):
        """æµ‹è¯•åˆ›å»ºæ–°ä¼šè¯"""
        start = time.time()
        try:
            chat_id = self.client.create_chat()
            assert chat_id is not None
            TestSessionManagement.test_chat_id = chat_id

            duration = time.time() - start
            self._record_result(
                "test_create_chat", True, TestLevel.CRITICAL, duration,
                f"åˆ›å»ºä¼šè¯æˆåŠŸ: {chat_id[:8]}...",
                {"chat_id": chat_id}
            )
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_create_chat", False, TestLevel.CRITICAL,
                              duration, str(e))
            pytest.fail(f"test_create_chat failed: {e}")

    def test_get_chat_messages(self):
        """æµ‹è¯•è·å–ä¼šè¯æ¶ˆæ¯"""
        if not TestSessionManagement.test_chat_id:
            pytest.skip("éœ€è¦å…ˆåˆ›å»ºä¼šè¯")

        start = time.time()
        try:
            messages = self.client.get_chat_messages(TestSessionManagement.test_chat_id)
            assert isinstance(messages, list)

            duration = time.time() - start
            self._record_result(
                "test_get_chat_messages", True, TestLevel.STANDARD, duration,
                f"è·å–æ¶ˆæ¯æˆåŠŸ: {len(messages)} æ¡",
                {"message_count": len(messages)}
            )
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_get_chat_messages", False, TestLevel.STANDARD,
                              duration, str(e))
            pytest.fail(f"test_get_chat_messages failed: {e}")

    def test_update_chat_metadata(self):
        """æµ‹è¯•æ›´æ–°ä¼šè¯å…ƒæ•°æ®"""
        if not TestSessionManagement.test_chat_id:
            pytest.skip("éœ€è¦å…ˆåˆ›å»ºä¼šè¯")

        start = time.time()
        try:
            new_title = "æµ‹è¯•ä¼šè¯_RAG_2026"
            result = self.client.update_chat_metadata(
                TestSessionManagement.test_chat_id,
                new_title
            )

            duration = time.time() - start
            self._record_result(
                "test_update_chat_metadata", True, TestLevel.STANDARD, duration,
                f"æ›´æ–°å…ƒæ•°æ®æˆåŠŸ: {new_title}",
                {"title": new_title}
            )
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_update_chat_metadata", False, TestLevel.STANDARD,
                              duration, str(e))
            pytest.fail(f"test_update_chat_metadata failed: {e}")

    def test_delete_chat(self):
        """æµ‹è¯•åˆ é™¤ä¼šè¯"""
        if not TestSessionManagement.test_chat_id:
            pytest.skip("éœ€è¦å…ˆåˆ›å»ºä¼šè¯")

        start = time.time()
        try:
            result = self.client.delete_chat(TestSessionManagement.test_chat_id)

            duration = time.time() - start
            self._record_result(
                "test_delete_chat", True, TestLevel.CRITICAL, duration,
                f"åˆ é™¤ä¼šè¯æˆåŠŸ",
                {"chat_id": TestSessionManagement.test_chat_id}
            )
            TestSessionManagement.test_chat_id = None
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_delete_chat", False, TestLevel.CRITICAL,
                              duration, str(e))
            pytest.fail(f"test_delete_chat failed: {e}")

    def test_clear_all_chats(self):
        """æµ‹è¯•æ¸…é™¤æ‰€æœ‰ä¼šè¯"""
        start = time.time()
        try:
            # å…ˆåˆ›å»ºä¸€ä¸ªä¼šè¯
            chat_id = self.client.create_chat()

            # ç„¶åæ¸…é™¤æ‰€æœ‰
            result = self.client.clear_all_chats()
            deleted_count = result.get("deleted_count", 0)

            duration = time.time() - start
            passed = deleted_count > 0

            self._record_result(
                "test_clear_all_chats", passed, TestLevel.CRITICAL, duration,
                f"æ¸…é™¤ä¼šè¯æˆåŠŸ: {deleted_count} ä¸ª",
                {"deleted_count": deleted_count}
            )
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_clear_all_chats", False, TestLevel.CRITICAL,
                              duration, str(e))
            pytest.fail(f"test_clear_all_chats failed: {e}")


class TestRAGRetrievalMetrics:
    """åŸºäº MIRAGE åŸºå‡†çš„æ£€ç´¢æŒ‡æ ‡æµ‹è¯•"""
    
    @classmethod
    def setup_class(cls):
        cls.client = RAGTestClient()
        cls.results: List[TestResult] = []
    
    def _record_result(self, name: str, passed: bool, level: TestLevel,
                      duration: float, message: str = "", 
                      details: Dict = None, benchmark: Dict = None):
        TestRAGRetrievalMetrics.results.append(TestResult(
            name=name,
            passed=passed,
            level=level,
            duration=duration,
            message=message,
            details=details or {},
            benchmark_comparison=benchmark or {}
        ))
    
    def test_precision_recall(self):
        """æµ‹è¯• Precision@K å’Œ Recall@K (MIRAGE æ ‡å‡†)"""
        start = time.time()
        try:
            queries = [
                "æ–°åŠ å¡EPç­¾è¯è¦æ±‚",
                "ODIå¢ƒå¤–æŠ•èµ„å¤‡æ¡ˆ",
                "æ–°åŠ å¡å…¬å¸ç¨åŠ¡",
            ]
            
            all_precision = []
            all_recall = []
            
            for query in queries:
                result = self.client.test_rag(query, k=10)
                meta = result.get("retrieval_metadata", {})
                
                chunks = meta.get("total_chunks_retrieved", 0)
                sources = meta.get("unique_sources_count", 0)
                score_range = meta.get("score_range", {})
                
                # è®¡ç®—æŒ‡æ ‡
                precision = min(1.0, sources / 10) if chunks > 0 else 0
                recall = min(1.0, sources / 5)  # å‡è®¾å¹³å‡ 5 ä¸ªç›¸å…³æ–‡æ¡£
                
                all_precision.append(precision)
                all_recall.append(recall)
            
            avg_precision = statistics.mean(all_precision)
            avg_recall = statistics.mean(all_recall)
            
            duration = time.time() - start
            
            # ä¸ MIRAGE æ ‡å‡†å¯¹æ¯”
            benchmark = {
                "precision@10": {
                    "actual": avg_precision,
                    "standard": MIRAGE_BENCHMARKS["precision@10"],
                    "status": "âœ“" if avg_precision >= MIRAGE_BENCHMARKS["precision@10"] else "â–³"
                },
                "recall@10": {
                    "actual": avg_recall,
                    "standard": MIRAGE_BENCHMARKS["recall@10"],
                    "status": "âœ“" if avg_recall >= MIRAGE_BENCHMARKS["recall@10"] else "â–³"
                }
            }
            
            passed = avg_precision >= MIRAGE_BENCHMARKS["precision@10"] * 0.8
            
            self._record_result(
                "test_precision_recall", passed, TestLevel.BENCHMARK, duration,
                f"Precision: {avg_precision:.3f}, Recall: {avg_recall:.3f}",
                {"precision": avg_precision, "recall": avg_recall},
                benchmark
            )
            
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_precision_recall", False, TestLevel.BENCHMARK,
                              duration, str(e))
            pytest.fail(f"test_precision_recall failed: {e}")
    
    def test_mrr(self):
        """æµ‹è¯• MRR (Mean Reciprocal Rank) - MIRAGE æ ¸å¿ƒæŒ‡æ ‡"""
        start = time.time()
        try:
            queries = [
                "åˆ¶é€ ä¸šå‡ºæµ·ä¸œå—äºš",
                "æ–°åŠ å¡æŠ•èµ„ä¼˜åŠ¿",
                "EPå‡†è¯ç”³è¯·æ¡ä»¶",
            ]
            
            mrr_scores = []
            
            for query in queries:
                result = self.client.test_rag(query, k=10)
                meta = result.get("retrieval_metadata", {})
                
                # ç®€åŒ– MRR è®¡ç®—
                sources = meta.get("unique_sources_count", 0)
                if sources > 0:
                    mrr_scores.append(1.0 / sources)
                else:
                    mrr_scores.append(0.0)
            
            avg_mrr = statistics.mean(mrr_scores)
            
            duration = time.time() - start
            
            benchmark = {
                "mrr": {
                    "actual": avg_mrr,
                    "standard": MIRAGE_BENCHMARKS["mrr"],
                    "status": "âœ“" if avg_mrr >= MIRAGE_BENCHMARKS["mrr"] else "â–³"
                }
            }
            
            passed = avg_mrr >= MIRAGE_BENCHMARKS["mrr"] * 0.8
            
            self._record_result(
                "test_mrr", passed, TestLevel.BENCHMARK, duration,
                f"MRR: {avg_mrr:.3f} (æ ‡å‡†: {MIRAGE_BENCHMARKS['mrr']})",
                {"mrr": avg_mrr},
                benchmark
            )
            
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_mrr", False, TestLevel.BENCHMARK, duration, str(e))
            pytest.fail(f"test_mrr failed: {e}")
    
    def test_ndcg(self):
        """æµ‹è¯• NDCG@K (Normalized Discounted Cumulative Gain)"""
        start = time.time()
        try:
            queries = [
                "æ–°åŠ å¡EPç­¾è¯",
                "ODIå¤‡æ¡ˆæµç¨‹",
                "å…¬å¸æ³¨å†ŒæŒ‡å—",
            ]
            
            ndcg_scores = []
            
            for query in queries:
                result = self.client.test_rag(query, k=10)
                sources = result.get("sources", [])
                
                # ç®€åŒ– NDCG è®¡ç®—
                if sources:
                    dcg = sum(1.0 / (i + 1) for i in range(min(10, len(sources))))
                    idcg = sum(1.0 / (i + 1) for i in range(min(10, len(sources))))
                    ndcg = dcg / idcg if idcg > 0 else 0
                    ndcg_scores.append(ndcg)
                else:
                    ndcg_scores.append(0)
            
            avg_ndcg = statistics.mean(ndcg_scores)
            
            duration = time.time() - start
            
            benchmark = {
                "ndcg@10": {
                    "actual": avg_ndcg,
                    "standard": MIRAGE_BENCHMARKS["ndcg@10"],
                    "status": "âœ“" if avg_ndcg >= MIRAGE_BENCHMARKS["ndcg@10"] else "â–³"
                }
            }
            
            passed = avg_ndcg >= MIRAGE_BENCHMARKS["ndcg@10"] * 0.8
            
            self._record_result(
                "test_ndcg", passed, TestLevel.BENCHMARK, duration,
                f"NDCG@10: {avg_ndcg:.3f}",
                {"ndcg": avg_ndcg},
                benchmark
            )
            
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_ndcg", False, TestLevel.BENCHMARK, duration, str(e))
            pytest.fail(f"test_ndcg failed: {e}")


class TestRAGAnswerQuality:
    """åŸºäº RAGBench çš„å›ç­”è´¨é‡æµ‹è¯•"""
    
    @classmethod
    def setup_class(cls):
        cls.client = RAGTestClient()
        cls.results: List[TestResult] = []
    
    def _record_result(self, name: str, passed: bool, level: TestLevel,
                      duration: float, message: str = "", 
                      details: Dict = None, benchmark: Dict = None):
        TestRAGRetrievalMetrics.results.append(TestResult(
            name=name,
            passed=passed,
            level=level,
            duration=duration,
            message=message,
            details=details or {},
            benchmark_comparison=benchmark or {}
        ))
    
    def test_answer_length_quality(self):
        """æµ‹è¯•å›ç­”é•¿åº¦åˆç†æ€§"""
        start = time.time()
        try:
            queries = [
                "æ–°åŠ å¡EPç­¾è¯è¦æ±‚æ˜¯ä»€ä¹ˆ",
                "ODIå¢ƒå¤–æŠ•èµ„å¤‡æ¡ˆæµç¨‹",
                "æ–°åŠ å¡å…¬å¸ç¨åŠ¡æœ‰å“ªäº›",
            ]
            
            lengths = []
            for query in queries:
                result = self.client.test_rag(query)
                answer = result.get("answer", "")
                lengths.append(len(answer))
            
            avg_length = statistics.mean(lengths)
            
            # åˆç†èŒƒå›´: 100-5000 å­—ç¬¦
            quality_score = 1.0 if 100 <= avg_length <= 5000 else 0.5
            
            duration = time.time() - start
            
            self._record_result(
                "test_answer_length_quality", quality_score > 0.5, 
                TestLevel.STANDARD, duration,
                f"å¹³å‡å›ç­”é•¿åº¦: {avg_length:.0f} å­—ç¬¦",
                {"avg_length": avg_length, "quality_score": quality_score}
            )
            
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_answer_length_quality", False, 
                              TestLevel.STANDARD, duration, str(e))
            pytest.fail(f"test_answer_length_quality failed: {e}")
    
    def test_context_grounding(self):
        """æµ‹è¯•å›ç­”çš„äº‹å®ä¾æ® (Grounding)"""
        start = time.time()
        try:
            query = "æ–°åŠ å¡EPç­¾è¯è¦æ±‚"
            result = self.client.test_rag(query)
            
            sources = result.get("sources", [])
            answer = result.get("answer", "")
            
            # æ£€æŸ¥å›ç­”æ˜¯å¦æœ‰ä¸Šä¸‹æ–‡æ”¯æŒ
            has_grounding = len(sources) > 0 and len(answer) > 100
            
            duration = time.time() - start
            
            benchmark = {
                "grounding_score": {
                    "actual": 1.0 if has_grounding else 0.0,
                    "standard": MIRAGE_BENCHMARKS["grounding_score"],
                    "status": "âœ“" if has_grounding else "â–³"
                }
            }
            
            self._record_result(
                "test_context_grounding", has_grounding, TestLevel.BENCHMARK, 
                duration,
                f"ä¸Šä¸‹æ–‡æ”¯æŒ: {has_grounding}, æ¥æºæ•°: {len(sources)}",
                {"has_grounding": has_grounding, "sources_count": len(sources)},
                benchmark
            )
            
        except Exception as e:
            duration = time.time() - start
            self._record_result("test_context_grounding", False, 
                              TestLevel.BENCHMARK, duration, str(e))
            pytest.fail(f"test_context_grounding failed: {e}")


class TestRAGPerformance:
    """RAG æ€§èƒ½æµ‹è¯•"""
    
    @classmethod
    def setup_class(cls):
        cls.client = RAGTestClient()
        cls.results: List[TestResult] = []
    
    def _record_result(self, name: str, passed: bool, level: TestLevel,
                      duration: float, message: str = "", 
                      details: Dict = None):
        TestRAGPerformance.results.append(TestResult(
            name=name,
            passed=passed,
            level=level,
            duration=duration,
            message=message,
            details=details or {}
        ))
    
    def test_retrieval_latency(self):
        """æµ‹è¯•æ£€ç´¢å»¶è¿Ÿ"""
        start = time.time()
        
        query = "æ–°åŠ å¡EPç­¾è¯"
        result = self.client.test_rag(query, k=10)
        
        duration = time.time() - start
        
        # æ ‡å‡†: < 5s
        passed = duration < 5.0
        
        self._record_result(
            "test_retrieval_latency", passed, TestLevel.STANDARD, duration,
            f"æ£€ç´¢å»¶è¿Ÿ: {duration:.2f}ç§’",
            {"latency": duration, "threshold": 5.0}
        )
    
    def test_cache_performance(self):
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½æå‡"""
        # ç¬¬ä¸€æ¬¡æŸ¥è¯¢ (æ— ç¼“å­˜)
        query = "æµ‹è¯•ç¼“å­˜æ€§èƒ½"
        start = time.time()
        self.client.test_llamaindex_rag(query, use_cache=False)
        first_duration = time.time() - start
        
        # ç¬¬äºŒæ¬¡æŸ¥è¯¢ (æœ‰ç¼“å­˜)
        start = time.time()
        self.client.test_llamaindex_rag(query, use_cache=True)
        cached_duration = time.time() - start
        
        # è®¡ç®—æ€§èƒ½æå‡
        speedup = first_duration / cached_duration if cached_duration > 0 else 1.0
        
        passed = speedup > 2.0  # è‡³å°‘ 2 å€æå‡
        
        self._record_result(
            "test_cache_performance", passed, TestLevel.STANDARD,
            cached_duration,
            f"é¦–æ¬¡: {first_duration:.2f}s, ç¼“å­˜: {cached_duration:.2f}s, æå‡: {speedup:.1f}x",
            {"first_duration": first_duration, "cached_duration": cached_duration, 
             "speedup": speedup}
        )


# ============================================================
# Test Report Generator
# ============================================================

class TestReportGenerator:
    """ä¸“ä¸šæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_report(results: List[TestResult]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("RAG System Professional Test Report")
        report.append("åŸºäº MIRAGE (ACL 2025) & RAGBench æ ‡å‡†")
        report.append("=" * 80)
        report.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().isoformat()}")
        report.append("")
        
        # æŒ‰çº§åˆ«åˆ†ç»„
        critical = [r for r in results if r.level == TestLevel.CRITICAL]
        standard = [r for r in results if r.level == TestLevel.STANDARD]
        benchmark = [r for r in results if r.level == TestLevel.BENCHMARK]
        
        # ç»Ÿè®¡
        total = len(results)
        passed = sum(1 for r in results if r.passed)
        failed = total - passed
        
        report.append(f"æ€»è®¡: {total} | é€šè¿‡: {passed} | å¤±è´¥: {failed} | æˆåŠŸç‡: {passed/total*100:.1f}%")
        report.append("")
        
        # å…³é”®åŠŸèƒ½æµ‹è¯•
        if critical:
            report.append("ã€å…³é”®åŠŸèƒ½æµ‹è¯•ã€‘")
            report.append("-" * 80)
            for r in critical:
                status = "âœ“ PASS" if r.passed else "âœ— FAIL"
                report.append(f"  {status} [{r.duration:.2f}s] {r.name}")
                if r.message:
                    report.append(f"         â†’ {r.message}")
            report.append("")
        
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        if benchmark:
            report.append("ã€æ€§èƒ½åŸºå‡†æµ‹è¯• - MIRAGE/RAGBench æ ‡å‡†ã€‘")
            report.append("-" * 80)
            for r in benchmark:
                status = "âœ“ PASS" if r.passed else "âœ— FAIL"
                report.append(f"  {status} [{r.duration:.2f}s] {r.name}")
                if r.benchmark_comparison:
                    for metric, data in r.benchmark_comparison.items():
                        actual = data.get("actual", 0)
                        std = data.get("standard", 0)
                        status_mark = data.get("status", "")
                        report.append(f"         {metric}: {actual:.3f} (æ ‡å‡†: {std}) {status_mark}")
            report.append("")
        
        # æ ‡å‡†æµ‹è¯•
        if standard:
            report.append("ã€æ ‡å‡†æ€§èƒ½æµ‹è¯•ã€‘")
            report.append("-" * 80)
            for r in standard:
                status = "âœ“ PASS" if r.passed else "âœ— FAIL"
                report.append(f"  {status} [{r.duration:.2f}s] {r.name}")
                if r.message:
                    report.append(f"         â†’ {r.message}")
            report.append("")
        
        # æ€»ç»“
        report.append("=" * 80)
        report.append("SUMMARY")
        report.append("=" * 80)
        
        if failed == 0:
            report.append("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        else:
            report.append(f"âš ï¸  {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
        
        return "\n".join(report)


# ============================================================
# Pytest Hooks
# ============================================================

def pytest_sessionfinish(session, exitstatus):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    all_results = []

    # æ”¶é›†æ‰€æœ‰æµ‹è¯•ç»“æœ
    for test_class in [TestRAGSystem, TestSessionManagement, TestRAGRetrievalMetrics,
                       TestRAGAnswerQuality, TestRAGPerformance]:
        if hasattr(test_class, 'results'):
            all_results.extend(test_class.results)

    if all_results:
        print("\n" + TestReportGenerator.generate_report(all_results))


# ============================================================
# Main Entry Point
# ============================================================

def main():
    """ä¸»å…¥å£"""
    import sys
    
    exit_code = pytest.main([
        __file__,
        "-v",
        "--tb=short",
        "-s",
        "--color=yes"
    ])
    
    print("\n" + "=" * 80)
    print("Professional Test Suite Completed")
    print("=" * 80)
    
    return exit_code


if __name__ == "__main__":
    main()
