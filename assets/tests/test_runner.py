#!/usr/bin/env python3
"""
RAG System Professional Test Runner
====================================

ä¸“ä¸šæµ‹è¯•è¿è¡Œå™¨ï¼ŒåŸºäº MIRAGE å’Œ RAGBench æ ‡å‡†ç”Ÿæˆè¯¦ç»†æµ‹è¯•æŠ¥å‘Š

å‚è€ƒæ ‡å‡†:
- MIRAGE: https://arxiv.org/abs/2504.17137
- RAGBench: https://arxiv.org/abs/2407.11005

ä½¿ç”¨æ–¹æ³•:
    python test_runner.py                    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    python test_runner.py --compare         # è¡Œä¸šæ ‡å‡†å¯¹æ¯”
    python test_runner.py --quick           # å¿«é€Ÿæµ‹è¯•
    python test_runner.py --performance    # æ€§èƒ½æµ‹è¯•
    python test_runner.py --full           # å®Œæ•´æµ‹è¯• (é»˜è®¤)
    python test_runner.py --history        # æŸ¥çœ‹å†å²è®°å½•
    python test_runner.py --diff           # å¯¹æ¯”ä¸Šæ¬¡æµ‹è¯•
    python test_runner.py --diff N         # å¯¹æ¯”ç¬¬ N æ¬¡æµ‹è¯•
"""

import requests
import json
import time
import statistics
import sys
import os
from datetime import datetime
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict


# ============================================================
# Configuration
# ============================================================

BACKEND_URL = "http://localhost:8000"

# è¡Œä¸šæ ‡å‡† (MIRAGE, RAGBench)
INDUSTRY_STANDARDS = {
    "precision@10": 0.6,
    "recall@10": 0.5,
    "mrr": 0.65,
    "ndcg@10": 0.55,
    "latency_p95": 5000,  # ms
    "cache_speedup": 10,   # æœ€ä½ 10x
}


# ============================================================
# Data Classes
# ============================================================

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    query: str
    chunks: int
    sources: int
    scores: List[float]
    answer_length: int
    duration: float
    source_names: List[str]


@dataclass
class BenchmarkComparison:
    """åŸºå‡†å¯¹æ¯”"""
    metric: str
    actual: float
    standard: float
    status: str  # "pass", "fail", "warning"


# ============================================================
# History & Storage
# ============================================================

HISTORY_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "test_history.json")


def save_test_result(results: Dict[str, Any]):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°å†å²è®°å½•"""
    # åŠ è½½ç°æœ‰å†å²
    history = []
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                history = json.load(f)
        except Exception:
            history = []
    
    # åˆ›å»ºæ–°è®°å½• (ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡)
    record = {
        "timestamp": datetime.now().isoformat(),
        "results": {
            "total_entities": results.get("total_entities", 0),
            "source_count": results.get("source_count", (0, 0)),
            "benchmark": results.get("benchmark", {}),
            "performance": results.get("performance", {}),
            "domain": results.get("domain", {}),
            "overall_score": results.get("overall_score", 0),
        }
    }
    
    # æ·»åŠ åˆ°å†å² (ä¿ç•™æœ€è¿‘ 20 æ¡)
    history.append(record)
    history = history[-20:]
    
    # ä¿å­˜
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    
    print(f"\n  ğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {HISTORY_FILE}")


def show_history():
    """æ˜¾ç¤ºå†å²è®°å½•æ¦‚è§ˆ"""
    if not os.path.exists(HISTORY_FILE):
        print("\n  ğŸ“­ æš‚æ— å†å²æµ‹è¯•è®°å½•")
        return
    
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            history = json.load(f)
    except Exception:
        print("\n  ğŸ“­ æ— æ³•è¯»å–å†å²è®°å½•")
        return
    
    if not history:
        print("\n  ğŸ“­ æš‚æ— å†å²æµ‹è¯•è®°å½•")
        return
    
    print("\n" + "â•" * 70)
    print("  å†å²æµ‹è¯•è®°å½•")
    print("â•" * 70)
    
    print(f"\n  {'#':<3} {'æ—¶é—´':<20} {'ç»¼åˆå¾—åˆ†':<10} {'MRR':<8} {'ç¼“å­˜åŠ é€Ÿ':<10}")
    print("  " + "â”€" * 70)
    
    for i, record in enumerate(reversed(history), 1):
        timestamp = record.get("timestamp", "")[:19]
        results = record.get("results", {})
        score = results.get("overall_score", 0)
        mrr = results.get("benchmark", {}).get("mrr", {}).get("actual", 0)
        speedup = results.get("performance", {}).get("speedup", 0)
        
        print(f"  {len(history)-i+1:<3} {timestamp:<20} {score:>6.1f}%    {mrr:.3f}   {speedup:.1f}x")
    
    print("\n  ä½¿ç”¨ --diff æŸ¥çœ‹ä¸ä¸Šæ¬¡çš„å¯¹æ¯”")
    print("  ä½¿ç”¨ --diff N æŸ¥çœ‹ä¸ç¬¬ N æ¬¡çš„å¯¹æ¯”")


def show_diff(target: Optional[int] = None):
    """å¯¹æ¯”æµ‹è¯•ç»“æœ"""
    if not os.path.exists(HISTORY_FILE):
        print("\n  ğŸ“­ å†å²è®°å½•ä¸è¶³ï¼Œæ— æ³•å¯¹æ¯”")
        return
    
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            history = json.load(f)
    except Exception:
        print("\n  ğŸ“­ æ— æ³•è¯»å–å†å²è®°å½•")
        return
    
    if len(history) < 2:
        print("\n  ğŸ“­ å†å²è®°å½•ä¸è¶³ï¼Œæ— æ³•å¯¹æ¯”")
        return
    
    # è·å–å½“å‰å’Œä¸Šæ¬¡ç»“æœ
    current = history[-1]
    if target is not None:
        if target < 1 or target > len(history):
            print(f"\n  âŒ æ— æ•ˆçš„è®°å½•ç¼–å· (1-{len(history)})")
            return
        previous = history[-(target + 1)]
        target_label = f"ç¬¬ {target} æ¬¡æµ‹è¯•"
    else:
        previous = history[-2]
        target_label = "ä¸Šæ¬¡æµ‹è¯•"
    
    curr_results = current.get("results", {})
    prev_results = previous.get("results", {})
    
    print("\n" + "â•" * 70)
    print(f"  æµ‹è¯•å¯¹æ¯”: æœ¬æ¬¡ vs {target_label}")
    print("â•" * 70)
    print(f"\n  å½“å‰: {current.get('timestamp', '')[:19]}")
    print(f"  å¯¹æ¯”: {previous.get('timestamp', '')[:19]}")
    
    # å¯¹æ¯”å…³é”®æŒ‡æ ‡
    print("\n  ã€å…³é”®æŒ‡æ ‡å¯¹æ¯”ã€‘")
    print(f"\n  {'æŒ‡æ ‡':<20} {'æœ¬æ¬¡':<12} {'ä¸Šæ¬¡':<12} {'å˜åŒ–':<10}")
    print("  " + "â”€" * 60)
    
    # ç»¼åˆå¾—åˆ†
    curr_score = curr_results.get("overall_score", 0)
    prev_score = prev_results.get("overall_score", 0)
    diff = curr_score - prev_score
    arrow = "â†‘" if diff > 0 else ("â†“" if diff < 0 else "=")
    print(f"  {'ç»¼åˆå¾—åˆ†':<18} {curr_score:>6.1f}%     {prev_score:>6.1f}%     {arrow} {abs(diff):.1f}%")
    
    # MRR
    curr_mrr = curr_results.get("benchmark", {}).get("mrr", {}).get("actual", 0)
    prev_mrr = prev_results.get("benchmark", {}).get("mrr", {}).get("actual", 0)
    diff = curr_mrr - prev_mrr
    arrow = "â†‘" if diff > 0 else ("â†“" if diff < 0 else "=")
    print(f"  {'MRR':<18} {curr_mrr:>9.3f}   {prev_mrr:>9.3f}   {arrow} {abs(diff):.3f}")
    
    # ç¼“å­˜åŠ é€Ÿ
    curr_cache = curr_results.get("performance", {}).get("speedup", 0)
    prev_cache = prev_results.get("performance", {}).get("speedup", 0)
    diff = curr_cache - prev_cache
    arrow = "â†‘" if diff > 0 else ("â†“" if diff < 0 else "=")
    print(f"  {'ç¼“å­˜åŠ é€Ÿ':<18} {curr_cache:>8.1f}x    {prev_cache:>8.1f}x    {arrow} {abs(diff):.1f}x")
    
    # P95 å»¶è¿Ÿ
    curr_lat = curr_results.get("performance", {}).get("p95_latency", 0)
    prev_lat = prev_results.get("performance", {}).get("p95_latency", 0)
    diff = curr_lat - prev_lat
    arrow = "â†‘" if diff > 0 else ("â†“" if diff < 0 else "=")
    print(f"  {'P95 å»¶è¿Ÿ':<18} {curr_lat:>7.0f}ms    {prev_lat:>7.0f}ms    {arrow} {abs(diff):.0f}ms")
    
    # å‘é‡æ•°
    curr_vec = curr_results.get("total_entities", 0)
    prev_vec = prev_results.get("total_entities", 0)
    diff = curr_vec - prev_vec
    arrow = "â†‘" if diff > 0 else ("â†“" if diff < 0 else "=")
    print(f"  {'å‘é‡æ€»æ•°':<18} {curr_vec:>9}    {prev_vec:>9}    {arrow} {abs(diff)}")
    
    print("\n" + "â•" * 70)


# ============================================================
# Test Functions
# ============================================================

def print_header(title: str, width: int = 80):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "â•" * width)
    print(f"  {title}")
    print("â•" * width)


def print_section(title: str):
    """æ‰“å°ç« èŠ‚"""
    print(f"\nâ”Œ{'â”€' * 60}â”")
    print(f"â”‚ {title:^58} â”‚")
    print(f"â””{'â”€' * 60}â”˜")


def print_metric_table(headers: List[str], rows: List[List[str]]):
    """æ‰“å°æŒ‡æ ‡è¡¨æ ¼"""
    col_widths = [max(len(str(row[i])) for row in [headers] + rows) for i in range(len(headers))]
    
    # è¡¨å¤´
    header_row = "â”‚" + "â”‚".join(f" {h:{col_widths[i]}} " for i, h in enumerate(headers)) + "â”‚"
    print("â”œ" + "â”¬".join("â”€" * (w + 2) for w in col_widths) + "â”¤")
    print(header_row)
    print("â”œ" + "â”¼".join("â”€" * (w + 2) for w in col_widths) + "â”¤")
    
    # æ•°æ®è¡Œ
    for row in rows:
        data_row = "â”‚" + "â”‚".join(f" {str(row[i]):{col_widths[i]}} " for i in range(len(row))) + "â”‚"
        print(data_row)
    
    print("â””" + "â”´".join("â”€" * (w + 2) for w in col_widths) + "â”˜")


# ============================================================
# Test Cases
# ============================================================

def test_vector_stats() -> Dict[str, Any]:
    """å‘é‡å­˜å‚¨ç»Ÿè®¡æµ‹è¯•"""
    print_section("å‘é‡å­˜å‚¨ç»Ÿè®¡")
    
    response = requests.get(f"{BACKEND_URL}/test/vector-stats", timeout=30)
    data = response.json()
    
    print(f"  Collection: {data.get('collection')}")
    print(f"  æ€»å‘é‡æ•°: {data.get('total_entities')}")
    print(f"  å­—æ®µæ•°: {len(data.get('fields', []))}")
    print(f"  ç´¢å¼•æ•°: {data.get('index_count')}")
    
    fields = [f["name"] for f in data.get("fields", [])]
    print(f"  å­—æ®µåˆ—è¡¨: {', '.join(fields)}")
    
    return data


def test_sources_management() -> tuple:
    """æ–‡æ¡£æºç®¡ç†æµ‹è¯•"""
    print_section("æ–‡æ¡£æºç®¡ç†")
    
    # è·å–æ‰€æœ‰æº
    response = requests.get(f"{BACKEND_URL}/sources", timeout=30)
    all_sources = response.json().get("sources", [])
    
    # è·å–é€‰ä¸­çš„æº
    response = requests.get(f"{BACKEND_URL}/selected_sources", timeout=30)
    selected_sources = response.json().get("sources", [])
    
    print(f"  æ€»æ–‡æ¡£æ•°: {len(all_sources)}")
    print(f"  å·²é€‰æ–‡æ¡£æ•°: {len(selected_sources)}")
    print(f"  æœªé€‰æ–‡æ¡£æ•°: {len(all_sources) - len(selected_sources)}")
    
    # æ˜¾ç¤ºå‰ 5 ä¸ªé€‰ä¸­æ–‡æ¡£
    if selected_sources:
        print("\n  å·²é€‰æ–‡æ¡£ (å‰5ä¸ª):")
        for i, src in enumerate(selected_sources[:5], 1):
            print(f"    {i}. {src[:60]}...")
    
    return len(all_sources), len(selected_sources)


def test_retrieval_metrics() -> List[RetrievalResult]:
    """æ£€ç´¢æŒ‡æ ‡æµ‹è¯•"""
    print_section("æ£€ç´¢è´¨é‡æµ‹è¯• - MIRAGE æ ‡å‡†")
    
    # æµ‹è¯•æŸ¥è¯¢ (è¦†ç›–ä¸åŒé¢†åŸŸ)
    queries = [
        "æ–°åŠ å¡EPç­¾è¯è¦æ±‚",
        "ODIå¢ƒå¤–æŠ•èµ„å¤‡æ¡ˆæµç¨‹",
        "æ–°åŠ å¡å…¬å¸ç¨åŠ¡",
        "åˆ¶é€ ä¸šå‡ºæµ·ä¸œå—äºš",
        "ACRAå…¬å¸æ³¨å†Œ",
    ]
    
    results = []
    
    print(f"\n  {'æŸ¥è¯¢':<25} {'Chunks':<10} {'Sources':<10} {'åˆ†æ•°èŒƒå›´':<20} {'è€—æ—¶':<10}")
    print("  " + "â”€" * 80)
    
    for query in queries:
        start = time.time()
        response = requests.get(
            f"{BACKEND_URL}/test/rag",
            params={"query": query, "k": 10},
            timeout=120
        )
        duration = time.time() - start
        
        data = response.json()
        meta = data.get("retrieval_metadata", {})
        sources = data.get("sources", [])
        
        # æå–åˆ†æ•°
        scores = []
        for src in sources:
            for chunk in src.get("chunks", []):
                scores.append(chunk.get("score", 0))
        
        result = RetrievalResult(
            query=query,
            chunks=meta.get("total_chunks_retrieved", 0),
            sources=meta.get("unique_sources_count", 0),
            scores=scores,
            answer_length=len(data.get("answer", "")),
            duration=duration,
            source_names=[s.get("name", "")[:30] for s in sources]
        )
        results.append(result)
        
        score_range = f"{min(scores):.3f}-{max(scores):.3f}" if scores else "N/A"
        print(f"  {query[:22]:<25} {result.chunks:<10} {result.sources:<10} {score_range:<20} {duration:.2f}s")
    
    return results


def test_benchmark_comparison(results: List[RetrievalResult]) -> List[BenchmarkComparison]:
    """è¡Œä¸šæ ‡å‡†å¯¹æ¯”"""
    print_section("è¡Œä¸šæ ‡å‡†å¯¹æ¯” - MIRAGE/RAGBench")
    
    comparisons = []
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_sources = statistics.mean(r.sources for r in results)
    avg_chunks = statistics.mean(r.chunks for r in results)
    
    # Precision@10
    precision = min(1.0, avg_sources / 10)
    comparisons.append(BenchmarkComparison(
        metric="Precision@10",
        actual=precision,
        standard=INDUSTRY_STANDARDS["precision@10"],
        status="pass" if precision >= INDUSTRY_STANDARDS["precision@10"] else "fail"
    ))
    
    # Recall@10
    recall = min(1.0, avg_sources / 5)
    comparisons.append(BenchmarkComparison(
        metric="Recall@10",
        actual=recall,
        standard=INDUSTRY_STANDARDS["recall@10"],
        status="pass" if recall >= INDUSTRY_STANDARDS["recall@10"] else "fail"
    ))
    
    # MRR
    mrr_scores = [1.0 / r.sources if r.sources > 0 else 0 for r in results]
    mrr = statistics.mean(mrr_scores)
    comparisons.append(BenchmarkComparison(
        metric="MRR",
        actual=mrr,
        standard=INDUSTRY_STANDARDS["mrr"],
        status="pass" if mrr >= INDUSTRY_STANDARDS["mrr"] else "fail"
    ))
    
    # NDCG@10
    ndcg_scores = []
    for r in results:
        if r.scores:
            dcg = sum(1.0 / (i + 1) for i in range(min(10, len(r.scores))))
            idcg = sum(1.0 / (i + 1) for i in range(min(10, len(r.scores))))
            ndcg = dcg / idcg if idcg > 0 else 0
            ndcg_scores.append(ndcg)
    ndcg = statistics.mean(ndcg_scores) if ndcg_scores else 0
    comparisons.append(BenchmarkComparison(
        metric="NDCG@10",
        actual=ndcg,
        standard=INDUSTRY_STANDARDS["ndcg@10"],
        status="pass" if ndcg >= INDUSTRY_STANDARDS["ndcg@10"] else "fail"
    ))
    
    # æ‰“å°å¯¹æ¯”è¡¨
    print("\n  æŒ‡æ ‡å¯¹æ¯”è¡¨:")
    print(f"\n  {'æŒ‡æ ‡':<20} {'å®æµ‹å€¼':<15} {'è¡Œä¸šæ ‡å‡†':<15} {'çŠ¶æ€':<10}")
    print("  " + "â”€" * 65)
    
    for comp in comparisons:
        status_icon = "âœ“" if comp.status == "pass" else ("â–³" if comp.status == "warning" else "âœ—")
        print(f"  {comp.metric:<20} {comp.actual:.3f}{'':<12} {comp.standard:.3f}{'':<12} {status_icon} {comp.status}")
    
    # è®¡ç®—é€šè¿‡ç‡
    passed = sum(1 for c in comparisons if c.status == "pass")
    total = len(comparisons)
    pass_rate = passed / total * 100
    
    print(f"\n  åŸºå‡†é€šè¿‡ç‡: {passed}/{total} ({pass_rate:.1f}%)")
    
    return comparisons


def test_performance() -> Dict[str, Any]:
    """æ€§èƒ½æµ‹è¯•"""
    print_section("æ€§èƒ½æµ‹è¯•")
    
    # 1. æ£€ç´¢å»¶è¿Ÿ
    print("\n  [1] æ£€ç´¢å»¶è¿Ÿæµ‹è¯•")
    
    latencies = []
    for i in range(5):
        start = time.time()
        requests.get(f"{BACKEND_URL}/test/rag", params={"query": "æµ‹è¯•"}, timeout=30)
        duration = (time.time() - start) * 1000
        latencies.append(duration)
    
    avg_latency = statistics.mean(latencies)
    p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies)
    
    print(f"    å¹³å‡å»¶è¿Ÿ: {avg_latency:.0f}ms")
    print(f"    P95 å»¶è¿Ÿ: {p95_latency:.0f}ms")
    
    latency_pass = p95_latency <= INDUSTRY_STANDARDS["latency_p95"]
    print(f"    çŠ¶æ€: {'âœ“ é€šè¿‡' if latency_pass else 'âœ— æœªé€šè¿‡'} (æ ‡å‡†: â‰¤{INDUSTRY_STANDARDS['latency_p95']}ms)")
    
    # 2. ç¼“å­˜æ€§èƒ½æµ‹è¯•
    print("\n  [2] ç¼“å­˜æ€§èƒ½æµ‹è¯•")
    
    query = "ç¼“å­˜æ€§èƒ½æµ‹è¯•ä¸“ç”¨æŸ¥è¯¢"
    
    # ç¬¬ä¸€æ¬¡æŸ¥è¯¢ (å†™å…¥ç¼“å­˜)
    start = time.time()
    requests.post(
        f"{BACKEND_URL}/rag/llamaindex/query",
        json={"query": query, "use_cache": True},
        timeout=30
    )
    first_time = time.time() - start
    
    # ç¬¬äºŒæ¬¡æŸ¥è¯¢ (å‘½ä¸­ç¼“å­˜)
    start = time.time()
    requests.post(
        f"{BACKEND_URL}/rag/llamaindex/query",
        json={"query": query, "use_cache": True},
        timeout=30
    )
    cached_time = time.time() - start
    
    speedup = first_time / cached_time if cached_time > 0 else 0
    
    print(f"    é¦–æ¬¡æŸ¥è¯¢: {first_time*1000:.0f}ms")
    print(f"    ç¼“å­˜æŸ¥è¯¢: {cached_time*1000:.0f}ms")
    print(f"    æ€§èƒ½æå‡: {speedup:.1f}x")
    
    cache_pass = speedup >= INDUSTRY_STANDARDS["cache_speedup"]
    print(f"    çŠ¶æ€: {'âœ“ é€šè¿‡' if cache_pass else 'âœ— æœªé€šè¿‡'} (æ ‡å‡†: â‰¥{INDUSTRY_STANDARDS['cache_speedup']}x)")
    
    return {
        "avg_latency": avg_latency,
        "p95_latency": p95_latency,
        "first_time": first_time,
        "cached_time": cached_time,
        "speedup": speedup
    }


def test_llamaindex_features() -> Dict[str, Any]:
    """LlamaIndex ç‰¹æ€§æµ‹è¯•"""
    print_section("LlamaIndex å¢å¼ºåŠŸèƒ½æµ‹è¯•")
    
    # 1. é…ç½®
    print("\n  [1] LlamaIndex é…ç½®")
    response = requests.get(f"{BACKEND_URL}/rag/llamaindex/config", timeout=10)
    config = response.json()
    print(f"    çŠ¶æ€: {config.get('status')}")
    features = config.get('features', {})
    for k, v in features.items():
        print(f"    {k}: {v}")
    
    # 2. ç»Ÿè®¡
    print("\n  [2] LlamaIndex ç»Ÿè®¡")
    response = requests.get(f"{BACKEND_URL}/rag/llamaindex/stats", timeout=10)
    stats = response.json()
    if "index" in stats:
        print(f"    å‘é‡æ€»æ•°: {stats['index'].get('total_entities')}")
        print(f"    åµŒå…¥ç»´åº¦: {stats['index'].get('embedding_dimensions')}")
        print(f"    ç¼“å­˜æŸ¥è¯¢æ•°: {stats['cache'].get('cached_queries')}")
    
    # 3. æŸ¥è¯¢æµ‹è¯•
    print("\n  [3] LlamaIndex æŸ¥è¯¢æµ‹è¯•")
    response = requests.post(
        f"{BACKEND_URL}/rag/llamaindex/query",
        json={"query": "æ–°åŠ å¡EP", "top_k": 3},
        timeout=30
    )
    result = response.json()
    print(f"    å“åº”: {'æˆåŠŸ' if 'answer' in result else 'å¤±è´¥'}")
    
    return {"config": config, "stats": stats}


def test_domain_specific() -> Dict[str, Any]:
    """é¢†åŸŸç‰¹å®šæµ‹è¯•"""
    print_section("é¢†åŸŸç‰¹å®šæµ‹è¯• - RAGBench åŸŸ")
    
    domains = {
        "finance": ["æ–°åŠ å¡å…¬å¸ç¨åŠ¡", "ODIå¢ƒå¤–æŠ•èµ„", "ACRAæ³¨å†Œ"],
        "legal": ["EPç­¾è¯è¦æ±‚", "PDPAåˆè§„", "é›‡ä½£æ³•è§„"],
        "technology": ["åˆ¶é€ ä¸šå‡ºæµ·", "ç§‘æŠ€æŠ•èµ„", "äººå·¥æ™ºèƒ½"]
    }
    
    results = {}
    
    for domain, queries in domains.items():
        print(f"\n  [{domain}]")
        
        domain_scores = []
        for query in queries:
            response = requests.get(
                f"{BACKEND_URL}/test/rag",
                params={"query": query, "k": 5},
                timeout=60
            )
            data = response.json()
            sources = data.get("retrieval_metadata", {}).get("unique_sources_count", 0)
            domain_scores.append(sources)
        
        avg_score = statistics.mean(domain_scores)
        results[domain] = avg_score
        print(f"    å¹³å‡æ¥æºæ•°: {avg_score:.1f}")
    
    return results


def generate_summary(
    total_entities: int,
    source_count: tuple,
    retrieval_results: List[RetrievalResult],
    benchmark_results: List[BenchmarkComparison],
    performance_results: Dict[str, Any],
    domain_results: Dict[str, Any]
):
    """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
    print_header("æµ‹è¯•æ‘˜è¦æŠ¥å‘Š")
    
    print(f"\n  æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å‘é‡å­˜å‚¨
    print("\n  ã€å‘é‡å­˜å‚¨ã€‘")
    print(f"    æ€»å‘é‡æ•°: {total_entities}")
    print(f"    é›†åˆåç§°: context")
    
    # æ–‡æ¡£æº
    print("\n  ã€æ–‡æ¡£æºã€‘")
    print(f"    æ€»æ–‡æ¡£æ•°: {source_count[0]}")
    print(f"    å·²é€‰æ–‡æ¡£: {source_count[1]}")
    
    # æ£€ç´¢è´¨é‡
    print("\n  ã€æ£€ç´¢è´¨é‡ - MIRAGE æ ‡å‡†ã€‘")
    for comp in benchmark_results:
        status = "âœ“" if comp.status == "pass" else "âœ—"
        print(f"    {status} {comp.metric}: {comp.actual:.3f} (æ ‡å‡†: {comp.standard:.3f})")
    
    # æ€§èƒ½
    print("\n  ã€æ€§èƒ½æŒ‡æ ‡ã€‘")
    print(f"    P95 å»¶è¿Ÿ: {performance_results['p95_latency']:.0f}ms")
    print(f"    ç¼“å­˜åŠ é€Ÿ: {performance_results['speedup']:.1f}x")
    
    # é¢†åŸŸè¦†ç›–
    print("\n  ã€é¢†åŸŸè¦†ç›– - RAGBenchã€‘")
    for domain, score in domain_results.items():
        print(f"    {domain}: {score:.1f} å¹³å‡æ¥æº")
    
    # æ€»ä½“è¯„åˆ†
    print("\n  ã€æ€»ä½“è¯„ä¼°ã€‘")
    
    # è®¡ç®—ç»¼åˆåˆ†æ•°
    benchmark_pass = sum(1 for c in benchmark_results if c.status == "pass")
    benchmark_score = benchmark_pass / len(benchmark_results) * 100
    
    perf_score = 100
    if performance_results['p95_latency'] > INDUSTRY_STANDARDS["latency_p95"]:
        perf_score -= 20
    if performance_results['speedup'] < INDUSTRY_STANDARDS["cache_speedup"]:
        perf_score -= 20
    
    overall_score = (benchmark_score + perf_score) / 2
    
    print(f"    åŸºå‡†æµ‹è¯•å¾—åˆ†: {benchmark_score:.1f}%")
    print(f"    æ€§èƒ½æµ‹è¯•å¾—åˆ†: {perf_score:.1f}%")
    print(f"    ç»¼åˆå¾—åˆ†: {overall_score:.1f}%")
    
    if overall_score >= 80:
        grade = "A"
        emoji = "ğŸ‰"
    elif overall_score >= 60:
        grade = "B"
        emoji = "âœ“"
    elif overall_score >= 40:
        grade = "C"
        emoji = "âš "
    else:
        grade = "D"
        emoji = "âœ—"
    
    print(f"\n    ç»¼åˆè¯„çº§: {emoji} {grade} ({overall_score:.0f}åˆ†)")
    
    print("\n" + "â•" * 80)
    print("  ALL TESTS COMPLETED")
    print("â•" * 80)


# ============================================================
# Main Entry Point
# ============================================================

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    mode = sys.argv[1] if len(sys.argv) > 1 else "full"
    
    # å¤„ç†å†å²è®°å½•å‘½ä»¤
    if mode == "--history":
        show_history()
        return 0
    
    if mode == "--diff":
        target = None
        if len(sys.argv) > 2:
            try:
                target = int(sys.argv[2])
            except ValueError:
                print("  âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„è®°å½•ç¼–å·")
                return 1
        show_diff(target)
        return 0
    
    # è¿è¡Œæµ‹è¯•
    print_header("RAG System Professional Test Runner")
    print("  åŸºäº MIRAGE (ACL 2025) & RAGBench æ ‡å‡†")
    print(f"\n  Backend: {BACKEND_URL}")
    print(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = {}
    
    # 1. å‘é‡å­˜å‚¨æµ‹è¯•
    vector_data = test_vector_stats()
    results['total_entities'] = vector_data.get('total_entities', 0)
    
    # 2. æ–‡æ¡£æºæµ‹è¯•
    source_count = test_sources_management()
    results['source_count'] = source_count
    
    if mode in ["full", "performance"]:
        # 3. æ£€ç´¢æŒ‡æ ‡æµ‹è¯•
        retrieval_results = test_retrieval_metrics()
        results['retrieval'] = retrieval_results
        
        # 4. åŸºå‡†å¯¹æ¯”
        benchmark_results = test_benchmark_comparison(retrieval_results)
        results['benchmark'] = benchmark_results
        
        # 5. æ€§èƒ½æµ‹è¯•
        performance_results = test_performance()
        results['performance'] = performance_results
        
        # 6. é¢†åŸŸæµ‹è¯•
        domain_results = test_domain_specific()
        results['domain'] = domain_results
    
    if mode in ["full", "quick"]:
        # 7. LlamaIndex ç‰¹æ€§
        llamaindex_results = test_llamaindex_features()
        results['llamaindex'] = llamaindex_results
    
    # ç”Ÿæˆæ‘˜è¦
    if mode == "full":
        # è®¡ç®—ç»¼åˆå¾—åˆ†å¹¶å‡†å¤‡ä¿å­˜æ•°æ®
        benchmark_pass = sum(1 for c in benchmark_results if c.status == "pass")
        benchmark_score = benchmark_pass / len(benchmark_results) * 100
        
        perf_score = 100
        if performance_results['p95_latency'] > INDUSTRY_STANDARDS["latency_p95"]:
            perf_score -= 20
        if performance_results['speedup'] < INDUSTRY_STANDARDS["cache_speedup"]:
            perf_score -= 20
        
        overall_score = (benchmark_score + perf_score) / 2
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        results['overall_score'] = overall_score
        results['benchmark'] = {
            comp.metric: {"actual": comp.actual, "standard": comp.standard, "status": comp.status}
            for comp in benchmark_results
        }
        
        generate_summary(
            results['total_entities'],
            results['source_count'],
            results['retrieval'],
            benchmark_results,
            results['performance'],
            results['domain']
        )
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        save_test_result(results)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
